# ğŸ“„ Intelligent Financial Compliance Assistant (RAG System)

An **enterprise-grade Retrieval-Augmented Generation (RAG) system** that allows users to upload financial/compliance documents (PDFs) and ask questions grounded strictly in those documents â€” with **source attribution, document scoping, lifecycle management, and audit-safe responses**.

This project is designed as a **real-world backend + ML systems showcase**, not just a demo.

---

## ğŸš€ Key Features

- ğŸ“„ **PDF Upload & OCR Support**  
  Handles both text-based and scanned PDFs using OCR fallback.

- ğŸ” **Retrieval-Augmented Question Answering (RAG)**  
  Answers are generated *only* from retrieved document chunks â€” no hallucinations.

- ğŸ§  **Local LLM Inference (Free)**  
  Uses **Google Flan-T5 (Small)** for on-prem / CPU-based inference.

- ğŸ“š **Vector Search with FAISS**  
  High-performance semantic search using dense embeddings.

- ğŸ· **Metadata-Aware Chunking**  
  Each chunk stores:
  - `doc_id`
  - `original_filename`
  - `chunk_id`

- ğŸ¯ **Scoped Querying**  
  Ask across:
  - All documents, or
  - A specific selected document

- ğŸ“Œ **Source Attribution**  
  Every answer returns:
  - Original document name
  - Chunk ID
  - Clean, sentence-complete excerpts

- ğŸ—‚ **Document Lifecycle Management**
  - Upload
  - List documents
  - Delete individual documents
  - Reset entire knowledge base
  - Time-based retention & cleanup

- ğŸ–¥ **Modular Frontend**
  - Streamlit UI
  - Fully decoupled from backend
  - Easily replaceable with React / Next.js

---

## ğŸ§  Tech Stack

### Backend
- **Python**
- **FastAPI**
- **Uvicorn**
- **LangChain (Runnable API)**
- **FAISS**
- **HuggingFace Transformers**
- **SentenceTransformers**
- **Tesseract OCR**

### Models
- **LLM:** `google/flan-t5-small`
- **Embeddings:** `intfloat/e5-small-v2`
- **(Optional Reranker):** `all-MiniLM-L6-v2`

### Frontend
- **Streamlit**

### Deployment
- **Backend:** Railway (VM-based, persistent disk)
- **Frontend:** Streamlit Community Cloud

---


## ğŸ”„ RAG Flow (Step-by-Step)

1. User uploads a PDF  
2. Text is extracted (OCR if needed)  
3. Text is chunked with overlap  
4. Metadata is attached per chunk  
5. Embeddings are generated  
6. Chunks are stored in FAISS  
7. User asks a question  
8. Relevant chunks are retrieved  
9. LLM answers using retrieved context only  
10. Sources are returned with clean excerpts  

---

## ğŸ§ª Quality & Safety Measures

- âœ… **E5 Query Normalization** (`query:` / `passage:` prefixes)
- âœ… **Adaptive Retrieval (`k`)**
- âœ… **Sentence-Complete Source Excerpts**
- âœ… **Explicit â€œNot Foundâ€ behavior**
- âœ… **No hallucinations outside context**

---

## ğŸ§¹ Retention & Cleanup

- Time-based retention policy (configurable TTL)
- Manual cleanup endpoint
- Safe FAISS rebuild strategy
- Explicit reset of entire knowledge base

---

## âš™ CI/CD (Optional but Supported)

This project is **CI/CD-ready** using **GitHub Actions**.

Typical pipeline:
- On push to `main`
- Install dependencies
- Run basic checks
- Deploy backend (Railway)
- Deploy frontend (Streamlit Cloud auto-sync)

> CI/CD is intentionally lightweight to avoid overengineering while remaining production-aligned.

---

## ğŸŒ Live Demo

ğŸš§ Deployment attempted on VM-based platforms (Railway) for persistent FAISS + transformer inference.

Due to free-tier image size limits for ML dependencies (PyTorch, Transformers), the live demo may be temporarily unavailable.

â–¶ï¸ **The system can be run locally or demonstrated on request.**  
â–¶ï¸ Architecture, code, and evaluation are fully production-aligned.


---

## â–¶ï¸ Run Locally

### Backend
```bash
uvicorn app.main:app --reload
```

### Frontend
```streamlit
run frontend/streamlit_app/app.py
```


