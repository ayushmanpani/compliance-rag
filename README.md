# üìÑ Intelligent Financial Compliance Assistant (RAG System)

An **enterprise-grade Retrieval-Augmented Generation (RAG) system** that allows users to upload financial/compliance documents (PDFs) and ask questions grounded strictly in those documents ‚Äî with **source attribution, document scoping, lifecycle management, and audit-safe responses**.

This project is designed as a **real-world backend + ML systems showcase**, not just a demo.

---

## üöÄ Key Features

- üìÑ **PDF Upload & OCR Support**  
  Handles both text-based and scanned PDFs using OCR fallback.

- üîç **Retrieval-Augmented Question Answering (RAG)**  
  Answers are generated *only* from retrieved document chunks ‚Äî no hallucinations.

- üß† **Local LLM Inference (Free)**  
  Uses **Google Flan-T5 (Small)** for on-prem / CPU-based inference.

- üìö **Vector Search with FAISS**  
  High-performance semantic search using dense embeddings.

- üè∑ **Metadata-Aware Chunking**  
  Each chunk stores:
  - `doc_id`
  - `original_filename`
  - `chunk_id`

- üéØ **Scoped Querying**  
  Ask across:
  - All documents, or
  - A specific selected document

- üìå **Source Attribution**  
  Every answer returns:
  - Original document name
  - Chunk ID
  - Clean, sentence-complete excerpts

- üóÇ **Document Lifecycle Management**
  - Upload
  - List documents
  - Delete individual documents
  - Reset entire knowledge base
  - Time-based retention & cleanup

- üñ• **Modular Frontend**
  - Streamlit UI
  - Fully decoupled from backend
  - Easily replaceable with React / Next.js

---

## üß† Tech Stack

### Backend
- **Python**
- **FastAPI**
- **Uvicorn**
- **LangChain (Runnable API)**
- **FAISS**
- **HuggingFace Transformers**
- **SentenceTransformers**
- **Tesseract OCR**

### Models
- **LLM:** `google/flan-t5-small`
- **Embeddings:** `intfloat/e5-small-v2`
- **(Optional Reranker):** `all-MiniLM-L6-v2`

### Frontend
- **Streamlit**

### Deployment
- **Backend:** Railway (VM-based, persistent disk)
- **Frontend:** Streamlit Community Cloud

---


## üîÑ RAG Flow (Step-by-Step)

1. User uploads a PDF  
2. Text is extracted (OCR if needed)  
3. Text is chunked with overlap  
4. Metadata is attached per chunk  
5. Embeddings are generated  
6. Chunks are stored in FAISS  
7. User asks a question  
8. Relevant chunks are retrieved  
9. LLM answers using retrieved context only  
10. Sources are returned with clean excerpts  

---

## üß™ Quality & Safety Measures

- ‚úÖ **E5 Query Normalization** (`query:` / `passage:` prefixes)
- ‚úÖ **Adaptive Retrieval (`k`)**
- ‚úÖ **Sentence-Complete Source Excerpts**
- ‚úÖ **Explicit ‚ÄúNot Found‚Äù behavior**
- ‚úÖ **No hallucinations outside context**

---

## üßπ Retention & Cleanup

- Time-based retention policy (configurable TTL)
- Manual cleanup endpoint
- Safe FAISS rebuild strategy
- Explicit reset of entire knowledge base

---

## ‚öô CI/CD (Optional but Supported)

This project is **CI/CD-ready** using **GitHub Actions**.

Typical pipeline:
- On push to `main`
- Install dependencies
- Run basic checks
- Deploy backend (Railway)
- Deploy frontend (Streamlit Cloud auto-sync)

> CI/CD is intentionally lightweight to avoid overengineering while remaining production-aligned.

---

## üåê Live Demo

üöß **Live demo link will be added here after deployment**

> The application can also be run locally or demonstrated on request.

---

## ‚ñ∂Ô∏è Run Locally

### Backend
```bash
uvicorn app.main:app --reload
```

### Frontend
```streamlit
run frontend/streamlit_app/app.py
```


